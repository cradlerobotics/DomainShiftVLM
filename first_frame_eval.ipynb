{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98799639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For NLP metrics\n",
    "from evaluation.benchmark import calculate_nlp_metrics, calculate_clip_scores\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63980ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLMs = [\"smolvlm2\", \"gemma3n\", \"blip2\", \"llama_vision\", \"mistral\", \"gemma\", \"qwen\", \"llava\"]\n",
    "data_type = \"3d\" # either \"real\" or \"3d\"\n",
    "segmentation_dir = f\"output/{data_type}/sam2_tracking/\"\n",
    "\n",
    "mask_dir = \"with_masks\" # either \"with_masks\" or \"without_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "df = pd.read_csv(f\"data/{data_type}/ground_truth.csv\", header=0, sep=\";\")\n",
    "idx = df['object_id'].tolist()\n",
    "captions = df['caption'].tolist()\n",
    "\n",
    "gt_captions = {}\n",
    "for i in range(len(idx)):\n",
    "    gt_captions[idx[i]] = captions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = {}\n",
    "for VLM in VLMs:\n",
    "# Load non-aggregated captions based on the VLM\n",
    "    captions_dir = f\"output/{data_type}/caption/{VLM}/{mask_dir}\"\n",
    "    caption_files = glob.glob(os.path.join(captions_dir, f\"all_captions.csv\"))\n",
    "    if not caption_files:\n",
    "        raise FileNotFoundError(f\"No caption files found in {captions_dir} for {VLM}\")\n",
    "    for file_path in caption_files:\n",
    "        captions = {}\n",
    "\n",
    "    try:\n",
    "        # Read CSV with delimiter as semicolon\n",
    "        df = pd.read_csv(file_path, sep=';', header=0)\n",
    "        # Convert to nested dictionary, first key is frame_idx, second key is object_idx\n",
    "        for _, row in df.iterrows():\n",
    "            if row['frame_idx'] not in captions:\n",
    "                captions[row['frame_idx']] = {}\n",
    "            captions[row['frame_idx']][row['object_idx']] = row['caption']\n",
    "        all_captions[VLM] = captions\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8bd815",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_frame0 = {}\n",
    "\n",
    "for VLM, captions_dict in all_captions.items():\n",
    "    cap_frame0[VLM] = {}\n",
    "    for frame, cap_dict in captions_dict.items():\n",
    "        for obj_idx, caption in cap_dict.items():\n",
    "            if obj_idx not in cap_frame0[VLM]:\n",
    "                cap_frame0[VLM][obj_idx] = {}\n",
    "            if frame == 0:\n",
    "                cap_frame0[VLM][obj_idx] = caption\n",
    "\n",
    "cap_frame0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CLIP scores for ground truth\n",
    "clip_scores_gt = calculate_clip_scores(segmentation_dir, gt_captions, masked=(mask_dir==\"with_masks\"))\n",
    "\n",
    "# Print CLIP scores and ground truth captions\n",
    "print(\"Ground Truth Captions and CLIP Scores:\")\n",
    "for obj_id, caption in gt_captions.items():\n",
    "    print(f\"Object ID: {obj_id}, Caption: {caption}, CLIP Score: {clip_scores_gt[obj_id]}\")\n",
    "\n",
    "clip_scores_gt = [i for i in list(clip_scores_gt.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CLIP scores per model\n",
    "clip_scores_vlm = {}\n",
    "for VLM in VLMs:\n",
    "    scores = calculate_clip_scores(segmentation_dir, cap_frame0[VLM], masked=(mask_dir==\"with_masks\"))\n",
    "    clip_scores_vlm[VLM] = [i for i in list(scores.values())]\n",
    "\n",
    "# Print the results\n",
    "print(\"Ground Truth CLIP Scores:\")\n",
    "print(clip_scores_gt)\n",
    "\n",
    "for VLM in VLMs:\n",
    "    print(f\"{VLM} CLIP Scores:\")\n",
    "    print(clip_scores_vlm[VLM])\n",
    "\n",
    "# Calculate mean and standard deviation of CLIP scores\n",
    "mean_gt = np.mean(clip_scores_gt)\n",
    "std_gt = np.std(clip_scores_gt)\n",
    "mean_vlm = {}\n",
    "std_vlm = {}\n",
    "for VLM in VLMs:\n",
    "    mean_vlm[VLM] = np.mean(clip_scores_vlm[VLM])\n",
    "    std_vlm[VLM] = np.std(clip_scores_vlm[VLM])\n",
    "\n",
    "# Print mean and standard deviation\n",
    "print(f\"Ground Truth CLIP Score: {mean_gt:.2f} ± {std_gt:.2f}\")\n",
    "for VLM in VLMs:\n",
    "    print(f\"{VLM} CLIP Score: {mean_vlm[VLM]:.2f} ± {std_vlm[VLM]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single figure for the bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.rcParams.update({'font.size': 14, 'axes.labelsize': 16, 'axes.titlesize': 18, 'xtick.labelsize': 14, 'ytick.labelsize': 14, 'legend.fontsize': 13})\n",
    "\n",
    "# Calculate average CLIP score for ground truth and each VLM\n",
    "avg_gt_score = np.mean(clip_scores_gt)\n",
    "avg_vlm_scores = {vlm: np.mean(scores).item() for vlm, scores in clip_scores_vlm.items()}\n",
    "\n",
    "# Bar plot of average CLIP scores\n",
    "models = ['Ground Truth'] + VLMs\n",
    "avg_scores = [avg_gt_score] + [avg_vlm_scores[vlm] for vlm in VLMs]\n",
    "std_scores = [np.std(clip_scores_gt)] + [np.std(scores) for scores in clip_scores_vlm.values()]\n",
    "min_scores = [np.min(clip_scores_gt)] + [np.min(scores) for scores in clip_scores_vlm.values()]\n",
    "max_scores = [np.max(clip_scores_gt)] + [np.max(scores) for scores in clip_scores_vlm.values()]\n",
    "\n",
    "# Custom colors with gray for ground truth and colorful for VLMs\n",
    "colors = [(0.5, 0.5, 0.5, 1.0)] + [plt.cm.tab10(i) for i in range(len(VLMs))]\n",
    "plt.rcParams.update({'font.size': 14, 'axes.labelsize': 16, 'axes.titlesize': 18, 'xtick.labelsize': 14, 'ytick.labelsize': 14, 'legend.fontsize': 13})\n",
    "\n",
    "# Create the bar plot\n",
    "ax.bar(models, avg_scores, yerr=std_scores, capsize=5, color=colors)\n",
    "# ax.set_title('Average CLIP Score by Model (with Std Dev)', fontsize=14)\n",
    "ax.set_ylabel('CLIP Score', fontsize=12)\n",
    "ax.set_ylim([.2, .38])\n",
    "ax.tick_params(axis='x')\n",
    "ax.axhline(y=avg_gt_score, color='black', linestyle='--', alpha=0.7, label='GT Average')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for VLM in VLMs:\n",
    "    clip_scores_vlm[VLM] = (np.mean(clip_scores_vlm[VLM]), np.std(clip_scores_vlm[VLM]), np.min(clip_scores_vlm[VLM]), np.max(clip_scores_vlm[VLM]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NLP metrics for all VLMs compared to ground truth\n",
    "\n",
    "all_nlp_metrics = {}\n",
    "nlp_metrics = {}\n",
    "\n",
    "for VLM in VLMs:\n",
    "    # Extract captions for this VLM (using frame 0)\n",
    "    all_nlp_metrics[VLM] = {}\n",
    "    vlm_captions = {}\n",
    "    for obj_idx, caption in cap_frame0[VLM].items():\n",
    "        vlm_captions[obj_idx] = caption\n",
    "    \n",
    "    # Calculate NLP metrics between this VLM's captions and ground truth\n",
    "    # Calculate NLP metrics between this VLM's captions and ground truth\n",
    "    raw_metrics = calculate_nlp_metrics(gt_captions, vlm_captions)\n",
    "    all_nlp_metrics[VLM] = raw_metrics.copy()\n",
    "    # print(raw_metrics)\n",
    "    # Calculate statistics for each metric\n",
    "    metrics = {}\n",
    "    for metric_name, values in raw_metrics.items():\n",
    "        values_list = list(values)\n",
    "\n",
    "        metrics[metric_name] = (\n",
    "            np.mean(values_list),  # mean\n",
    "            np.std(values_list),   # std\n",
    "            np.min(values_list),   # min\n",
    "            np.max(values_list)    # max\n",
    "        )\n",
    "        \n",
    "    nlp_metrics[VLM] = metrics\n",
    "\n",
    "# Create a DataFrame to present the results\n",
    "metrics_df = pd.DataFrame(columns=['VLM', 'CLIP', 'CIDEr', 'BERT', 'ROUGE', 'GPT'])\n",
    "\n",
    "for VLM, metrics in nlp_metrics.items():\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "        'VLM': [VLM],\n",
    "        'CLIP': [float(clip_scores_vlm[VLM][0])],\n",
    "        'CLIP_std': [float(clip_scores_vlm[VLM][1])],\n",
    "        'CLIP_min': [float(clip_scores_vlm[VLM][2])],\n",
    "        'CLIP_max': [float(clip_scores_vlm[VLM][3])],\n",
    "        'CIDEr': [float(metrics['cider'][0])],\n",
    "        'CIDEr_std': [float(metrics['cider'][1])],\n",
    "        'CIDEr_min': [float(metrics['cider'][2])],\n",
    "        'CIDEr_max': [float(metrics['cider'][3])],\n",
    "        'BERT': [float(metrics['bert'][0])],\n",
    "        'BERT_std': [float(metrics['bert'][1])],\n",
    "        'BERT_min': [float(metrics['bert'][2])],\n",
    "        'BERT_max': [float(metrics['bert'][3])],\n",
    "        'ROUGE': [float(metrics['rouge_l'][0])],\n",
    "        'ROUGE_std': [float(metrics['rouge_l'][1])],\n",
    "        'ROUGE_min': [float(metrics['rouge_l'][2])],\n",
    "        'ROUGE_max': [float(metrics['rouge_l'][3])],\n",
    "        'GPT': [float(metrics['gpt'][0])],\n",
    "        'GPT_std': [float(metrics['gpt'][1])],\n",
    "        'GPT_min': [float(metrics['gpt'][2])],\n",
    "        'GPT_max': [float(metrics['gpt'][3])], \n",
    "    })], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "metrics_df.to_csv(f'output/{data_type}/first_frame_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired VLM order for plotting\n",
    "vlm_order = ['smolvlm2', 'gemma3n', 'blip2', 'llama_vision', 'mistral', 'gemma', 'qwen', 'llava']\n",
    "\n",
    "metrics_to_plot = ['ROUGE', 'CIDEr', 'BERT', 'GPT']\n",
    "y_ranges = {\n",
    "    'CIDEr': (0, 6),\n",
    "    'GPT': None\n",
    "}\n",
    "default_ylim = (0, 1)\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    # Reorder metrics_df according to vlm_order\n",
    "    plot_df = metrics_df.set_index('VLM').loc[vlm_order].reset_index()\n",
    "    means = plot_df[metric]\n",
    "    stds = plot_df.get(f\"{metric}_std\", pd.Series([0]*len(plot_df)))\n",
    "    vlm_names = plot_df['VLM']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.bar(vlm_names, means, yerr=stds, capsize=5, color=plt.cm.tab10.colors[:len(vlm_names)])\n",
    "    ax.set_xticklabels(vlm_names, fontsize=12)\n",
    "    ax.set_ylabel(f'{metric} Score', fontsize=14)\n",
    "    ax.set_xlabel('VLM', fontsize=14)\n",
    "    ax.set_title(f'{metric} by VLM', fontsize=16)\n",
    "    if metric not in y_ranges:\n",
    "        ax.set_ylim(default_ylim)\n",
    "    else:\n",
    "        ax.set_ylim(y_ranges[metric])\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0115ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replot the metrics using seaborn swarmplot overlaid on a bar chart for each metric\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    plot_df = metrics_df.set_index('VLM').loc[vlm_order].reset_index()\n",
    "    means = plot_df[metric]\n",
    "    stds = plot_df.get(f\"{metric}_std\", pd.Series([0]*len(plot_df)))\n",
    "    vlm_names = plot_df['VLM']\n",
    "\n",
    "    # Prepare data for swarmplot\n",
    "    swarm_data = pd.DataFrame({\n",
    "        'VLM': np.repeat(vlm_names.values, 10),  # 10 objects per VLM\n",
    "        metric: np.concatenate([\n",
    "            all_nlp_metrics[vlm][metric.lower() if metric != \"ROUGE\" else \"rouge_l\"]\n",
    "            for vlm in vlm_order\n",
    "        ])\n",
    "    })\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # Bar chart (mean ± std)\n",
    "    ax.bar(vlm_names, means, yerr=stds, capsize=5, color=plt.cm.tab10.colors[:len(vlm_order)], label='Mean ± Std')\n",
    "    # Swarmplot\n",
    "    sns.swarmplot(data=swarm_data, x='VLM', y=metric, ax=ax, palette=plt.cm.tab10.colors[:len(vlm_order)], size=10, edgecolor='k', linewidth=0.5, alpha=0.5)\n",
    "    ax.set_ylabel(f'{metric} Score', fontsize=14)\n",
    "    ax.set_xlabel('VLM', fontsize=14)\n",
    "    ax.set_title(f'{metric} by VLM', fontsize=16)\n",
    "    if metric not in y_ranges:\n",
    "        ax.set_ylim(default_ylim)\n",
    "    else:\n",
    "        ax.set_ylim(y_ranges[metric])\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3709b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots for each metric across VLMs\n",
    "for metric in metrics_to_plot:\n",
    "    plot_df = metrics_df.set_index('VLM').loc[vlm_order].reset_index()\n",
    "    vlm_names = plot_df['VLM']\n",
    "\n",
    "    # Prepare data for boxplot\n",
    "    metric_key = metric.lower() if metric != \"ROUGE\" else \"rouge_l\"\n",
    "    box_data = []\n",
    "    for vlm in vlm_order:\n",
    "        values = all_nlp_metrics[vlm][metric_key]\n",
    "        box_data.append(values)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.boxplot(box_data, labels=vlm_names, patch_artist=True,\n",
    "               boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "               medianprops=dict(color='red'))\n",
    "    ax.set_ylabel(f'{metric} Score', fontsize=14)\n",
    "    ax.set_xlabel('VLM', fontsize=14)\n",
    "    ax.set_title(f'{metric} Boxplot by VLM', fontsize=16)\n",
    "    if metric not in y_ranges:\n",
    "        ax.set_ylim(default_ylim)\n",
    "    else:\n",
    "        if y_ranges[metric] is not None:\n",
    "            ax.set_ylim(y_ranges[metric])\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of low and high score captions for each metric and VLM\n",
    "def show_metric_examples(all_nlp_metrics, cap_frame0, gt_captions, n_examples=2):\n",
    "    \"\"\"\n",
    "    Show examples of low and high scoring captions for each metric and VLM\n",
    "    \"\"\"\n",
    "    metrics_to_analyze = ['bleu', 'rouge_l', 'meteor', 'cider', 'spice', 'bert', 'bleurt', 'bart', 'gpt']\n",
    "\n",
    "    for metric in metrics_to_analyze:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EXAMPLES FOR {metric.upper()}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        for vlm in VLMs:\n",
    "            print(f\"\\n{vlm.upper()}:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Get metric values and corresponding object indices\n",
    "            metric_values = all_nlp_metrics[vlm][metric]\n",
    "            obj_indices = list(range(len(metric_values)))\n",
    "            \n",
    "            # Sort by metric values to get lowest and highest\n",
    "            sorted_pairs = sorted(zip(obj_indices, metric_values), key=lambda x: x[1])\n",
    "            \n",
    "            # Get lowest examples\n",
    "            print(f\"LOWEST {metric.upper()} scores:\")\n",
    "            for i in range(min(n_examples, len(sorted_pairs))):\n",
    "                obj_idx, score = sorted_pairs[i]\n",
    "                gt_caption = gt_captions[obj_idx]\n",
    "                vlm_caption = cap_frame0[vlm][obj_idx]\n",
    "                print(f\"  Object {obj_idx} (Score: {score:.3f}):\")\n",
    "                print(f\"    Ground Truth: {gt_caption}\")\n",
    "                print(f\"    {vlm}: {vlm_caption}\")\n",
    "                print()\n",
    "            \n",
    "            # Get highest examples\n",
    "            print(f\"HIGHEST {metric.upper()} scores:\")\n",
    "            for i in range(min(n_examples, len(sorted_pairs))):\n",
    "                obj_idx, score = sorted_pairs[-(i+1)]\n",
    "                gt_caption = gt_captions[obj_idx]\n",
    "                vlm_caption = cap_frame0[vlm][obj_idx]\n",
    "                print(f\"  Object {obj_idx} (Score: {score:.3f}):\")\n",
    "                print(f\"    Ground Truth: {gt_caption}\")\n",
    "                print(f\"    {vlm}: {vlm_caption}\")\n",
    "                print()\n",
    "\n",
    "# Run the analysis\n",
    "show_metric_examples(all_nlp_metrics, cap_frame0, gt_captions, n_examples=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e715960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the examples data\n",
    "examples_data = []\n",
    "\n",
    "metrics_to_analyze = ['rouge_l', 'cider', 'bert', 'gpt']\n",
    "\n",
    "for metric in metrics_to_analyze:\n",
    "    for vlm in VLMs:\n",
    "        # Get metric values and corresponding object indices\n",
    "        metric_values = all_nlp_metrics[vlm][metric]\n",
    "        obj_indices = list(range(len(metric_values)))\n",
    "        \n",
    "        # Sort by metric values to get lowest and highest\n",
    "        sorted_pairs = sorted(zip(obj_indices, metric_values), key=lambda x: x[1])\n",
    "        \n",
    "        # Get lowest examples\n",
    "        for i in range(min(2, len(sorted_pairs))):\n",
    "            obj_idx, score = sorted_pairs[i]\n",
    "            gt_caption = gt_captions[obj_idx]\n",
    "            vlm_caption = cap_frame0[vlm][obj_idx]\n",
    "            examples_data.append({\n",
    "                'Metric': metric.upper(),\n",
    "                'VLM': vlm,\n",
    "                'Score_Type': 'LOWEST',\n",
    "                'Object_ID': obj_idx,\n",
    "                'Score': score,\n",
    "                'Ground_Truth': gt_caption,\n",
    "                'VLM_Caption': vlm_caption\n",
    "            })\n",
    "        \n",
    "        # Get highest examples\n",
    "        for i in range(min(2, len(sorted_pairs))):\n",
    "            obj_idx, score = sorted_pairs[-(i+1)]\n",
    "            gt_caption = gt_captions[obj_idx]\n",
    "            vlm_caption = cap_frame0[vlm][obj_idx]\n",
    "            examples_data.append({\n",
    "                'Metric': metric.upper(),\n",
    "                'VLM': vlm,\n",
    "                'Score_Type': 'HIGHEST',\n",
    "                'Object_ID': obj_idx,\n",
    "                'Score': score,\n",
    "                'Ground_Truth': gt_caption,\n",
    "                'VLM_Caption': vlm_caption\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "examples_df = pd.DataFrame(examples_data)\n",
    "\n",
    "# Save to CSV file\n",
    "examples_output_file = f\"output/{data_type}/metric_examples_{mask_dir}.csv\"\n",
    "examples_df.to_csv(examples_output_file, index=False, sep=';')\n",
    "print(f\"Metric examples saved to {examples_output_file}\")\n",
    "\n",
    "# Display summary of the examples table\n",
    "print(f\"\\nExamples table contains {len(examples_df)} rows\")\n",
    "print(f\"Metrics analyzed: {', '.join(examples_df['Metric'].unique())}\")\n",
    "print(f\"VLMs analyzed: {', '.join(examples_df['VLM'].unique())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
